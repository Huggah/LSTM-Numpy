{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Networks\n",
    "An LSTM Network, short for Long Short-Term Memory Network, is a type of recurrent neural network that can remember from timesteps long ago. A classic example of a classic RNN's shortcomings are predicting the next word in the sequence: I am French. (2000 words later) I speak fluent <i>French</i>. A classic RNN might predict any language, as it only remembers short term due to the vanishing gradient problem: since the hidden layer is constantly being multiplied by a value less than one, so the memory from timesteps deeper in the past are exponentially less significant.\n",
    "\n",
    "However, an LSTM network retains this memory through the use of a cell state: where long term memory is stored. Their ethnicity would be stored in the cell state, and it would predict the correct word. An LSTM's parameters tell it what to forget by element-wise multiplication, new things to remember by addition, and what parts of memory to select and pass on to the output of an LSTM cell, which, amazingly, can be learned effectively by performing an optimization technique such as RMSProp.\n",
    "\n",
    "These are done by gates, represented by rounded rectangles and their corresponding activation, including input (i), forget (f), input modulation (c tilde), and output (o) gates. Each gate is just a perceptron: with its own weights and biases. This LSTM has two weight matrices per gate: one for the input x, and another for the hidden state of the last layer. The hidden state, or output of the LSTM cell, is then passed into the final layer of the network, which will be the final output.\n",
    "\n",
    "This LSTM network is character-based: it tries to predict the next character in a sequence, where each character is represented by a 78 dimensional 1hot vector.\n",
    "\n",
    "![](http://www.mdpi.com/energies/energies-10-01168/article_deploy/html/images/energies-10-01168-g008.png)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return x * (1 - x)\n",
    "    \n",
    "def tanh(x, deriv=False):\n",
    "    if not deriv:\n",
    "        return np.tanh(x)\n",
    "    else:\n",
    "        return 1 - (x **2)\n",
    "    \n",
    "# Softmax function for output probabilities\n",
    "def softmax(x):\n",
    "    nonlin_x = np.exp(x)\n",
    "    return nonlin_x / np.sum(nonlin_x, 1).reshape(nonlin_x.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, learning_rate, input_dim, hidden_dim, output_dim):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initializing weight matrices\n",
    "        self.Wf = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.Wi = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.Wj = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.Wo = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        \n",
    "        # Initializing hidden state weight matrices\n",
    "        self.Uf = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.Ui = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.Uj = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.Uo = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        \n",
    "        # Initializing biases\n",
    "        self.Bf = np.random.randn(1, hidden_dim) * 0.01\n",
    "        self.Bi = np.random.randn(1, hidden_dim) * 0.01\n",
    "        self.Bj = np.random.randn(1, hidden_dim) * 0.01\n",
    "        self.Bo = np.random.randn(1, hidden_dim) * 0.01\n",
    "        \n",
    "        # Final layer parameters\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "        self.B2 = np.random.randn(1, output_dim) * 0.01\n",
    "        \n",
    "        # Memory: used in RMSProp optimizer\n",
    "        self.memory = [np.zeros([input_dim, hidden_dim]),\n",
    "                               np.zeros([input_dim, hidden_dim]), \n",
    "                               np.zeros([input_dim, hidden_dim]),\n",
    "                               np.zeros([input_dim, hidden_dim]),\n",
    "                               np.zeros([hidden_dim, output_dim]),\n",
    "                               np.zeros([1, hidden_dim]),\n",
    "                               np.zeros([1, hidden_dim]),\n",
    "                               np.zeros([1, hidden_dim]),\n",
    "                               np.zeros([1, hidden_dim]),\n",
    "                               np.zeros([1, output_dim]),\n",
    "                               np.zeros([hidden_dim, hidden_dim]), \n",
    "                               np.zeros([hidden_dim, hidden_dim]),\n",
    "                               np.zeros([hidden_dim, hidden_dim]),\n",
    "                               np.zeros([hidden_dim, hidden_dim])]\n",
    "        \n",
    "    # Getting a generated sample by predicting the next in a sequence\n",
    "    # over and over again, and adding the prediction to the sequence.\n",
    "    def sample(self, h_prev, c_prev, startchar, length=200):\n",
    "        Xs = [startchar]\n",
    "        prev_char = startchar\n",
    "        \n",
    "        # Getting previous cell and hidden states:\n",
    "        Cp = c_prev\n",
    "        Hp = h_prev\n",
    "        \n",
    "        for t in range(length):\n",
    "            # Forward pass\n",
    "            \n",
    "            X = np.zeros([1, self.input_dim])\n",
    "            X[:,prev_char] = 1\n",
    "            \n",
    "            # Gates\n",
    "            pf = np.dot(X, self.Wf) + np.dot(Hp, self.Uf) + self.Bf\n",
    "            f = sigmoid(pf)\n",
    "            pi = np.dot(X, self.Wi) + np.dot(Hp, self.Ui) + self.Bi\n",
    "            i = sigmoid(pi)\n",
    "            pj = np.dot(X, self.Wj) + np.dot(Hp, self.Uj) + self.Bj\n",
    "            j = tanh(pj)\n",
    "            po = np.dot(X, self.Wo) + np.dot(Hp, self.Uo) + self.Bo\n",
    "            o = sigmoid(po)\n",
    "            \n",
    "            # Cell state and hidden state operations\n",
    "            c = (Cp * f) + (i * j)\n",
    "            thc = tanh(c)\n",
    "            h = o * thc\n",
    "            Cp = c\n",
    "            Hp = h\n",
    "            \n",
    "            # Final layer logits\n",
    "            pyh = np.dot(h, self.W2) + self.B2\n",
    "            # Selecting the most probable outcome to be the\n",
    "            # next in the sequence\n",
    "            new_x = np.argmax(pyh)\n",
    "            \n",
    "            Xs.append(new_x)\n",
    "            prev_char = new_x\n",
    "            \n",
    "        return Xs\n",
    "       \n",
    "    # Computing gradients for the network\n",
    "    def compute_gradients(self, X, Y, h_prev, c_prev):\n",
    "        # Initialize cell and hidden states from the previous LSTM cell\n",
    "        # Since there is no previous cell, initialize them as 0\n",
    "        Cp = c_prev\n",
    "        Hp = h_prev\n",
    "        \n",
    "        # Error for keeping track of progress\n",
    "        error = 0\n",
    "        \n",
    "        # Initializing values inside the LSTM cell at every timestep that\n",
    "        # are necessary for computing gradients\n",
    "        Yh,H,C,O,I,J,F = [],[],[],[],[],[],[]\n",
    "        \n",
    "        # Forward pass: forward through time\n",
    "        for t in range(len(X)):\n",
    "            # Get gating values: J in this code represents the input modulation gate,\n",
    "            # or C tilde in the diagram above\n",
    "            f = sigmoid(np.dot(X[t:t+1], self.Wf) + np.dot(Hp, self.Uf) + self.Bf)\n",
    "            i = sigmoid(np.dot(X[t:t+1], self.Wi) + np.dot(Hp, self.Ui) + self.Bi)\n",
    "            j = tanh(np.dot(X[t:t+1], self.Wj) + np.dot(Hp, self.Uj) + self.Bj)\n",
    "            o = sigmoid(np.dot(X[t:t+1], self.Wo) + np.dot(Hp, self.Uo) + self.Bo)\n",
    "            \n",
    "            # Multiply the forget by the previous cell state and add the input:\n",
    "            # input gate * input modulation gate\n",
    "            c = (Cp * f) + (i * j)\n",
    "            # Hidden state: multiplying by output gate, selecting what to output\n",
    "            # from the LSTM cell\n",
    "            h = o * tanh(c)\n",
    "            # Setting states to current states for use in the next timestep\n",
    "            Cp = c\n",
    "            Hp = h\n",
    "            \n",
    "            # Passing hidden state through final layer producing output probabilities\n",
    "            pyh = np.dot(h, self.W2) + self.B2\n",
    "            yh = softmax(pyh)\n",
    "            \n",
    "            # Crossentropy error, not necessary, however, for computing gradients\n",
    "            error -= np.sum(Y[t] * np.log(yh))\n",
    "            \n",
    "            # Append values from forward pass for use in backpropagation and computing\n",
    "            # gradients\n",
    "            Yh.append(yh)\n",
    "            H.append(h)\n",
    "            C.append(c)\n",
    "            O.append(o)\n",
    "            I.append(i)\n",
    "            J.append(j)\n",
    "            F.append(f)\n",
    "        \n",
    "        # Backpropagation time: initialize derivatives of gate pre-activations as 0\n",
    "        dPF = np.zeros([1, self.hidden_dim])\n",
    "        dPI = np.zeros([1, self.hidden_dim])\n",
    "        dPJ = np.zeros([1, self.hidden_dim])\n",
    "        dPO = np.zeros([1, self.hidden_dim])\n",
    "        \n",
    "        # Initialize derivative of cell state with respect to previous cell state as 0\n",
    "        dCC = np.zeros([1, self.hidden_dim])\n",
    "        \n",
    "        # Initialize parameter gradients as 0\n",
    "        Wf_grad, Wi_grad, Wj_grad, Wo_grad, W2_grad = 0, 0, 0, 0, 0\n",
    "        Bf_grad, Bi_grad, Bj_grad, Bo_grad, B2_grad = 0, 0, 0, 0, 0\n",
    "        Uf_grad, Ui_grad, Uj_grad, Uo_grad = 0, 0, 0, 0\n",
    "        \n",
    "        # Backward pass through time\n",
    "        for t in reversed(range(len(X))):\n",
    "            # Deriv of crossentropy w.r.t. logits\n",
    "            dPYh = Yh[t] - Y[t]\n",
    "            \n",
    "            # Derivative of hidden state: it affects the logits at\n",
    "            # the current timestep and all the gating values of the cell\n",
    "            # at the next timestep, all affecting the total error\n",
    "            dH = (np.dot(dPYh, self.W2.T) + np.dot(dPF, self.Uf.T) + np.dot(dPI, self.Ui.T) + \n",
    "                  np.dot(dPJ, self.Uj.T) + np.dot(dPO, self.Uo.T))\n",
    "            \n",
    "            # Derivative of cell state: affects outcome of hidden state, as\n",
    "            # well as next cell state, which affects the next hidden state,\n",
    "            # and so on, represented by dCC: derivative of cell state with \n",
    "            # respect to previous cell state\n",
    "            dC = dH * O[t] * tanh(tanh(C[t]), deriv=True) + dCC\n",
    "            \n",
    "            # derivative of cell state with respect to previous cell state is the\n",
    "            # derivative of cell state * forget: using the chain rule through\n",
    "            # the formula for computing the next cell state\n",
    "            dCC = dC * F[t]\n",
    "            \n",
    "            # Use deriv of cell state to compute derivs of gate pre-activations\n",
    "            \n",
    "            if t != 0:\n",
    "                dPF = dC * C[t - 1] * sigmoid(F[t], deriv=True)\n",
    "            else:\n",
    "                dPF = np.zeros([1, hidden_dim])\n",
    "                \n",
    "            dPI = dC * J[t] * sigmoid(I[t], deriv=True)\n",
    "            dPJ = dC * I[t] * tanh(J[t], deriv=True)\n",
    "            \n",
    "            # Use deriv of hidden state to compute deriv of output gate\n",
    "            # pre-activations\n",
    "            dPO = dH * tanh(C[t]) * sigmoid(O[t], deriv=True)\n",
    "            \n",
    "            # Using deriv of gate pre-activations to compute weight gradients\n",
    "            Wo_grad += np.dot(X[t:t+1].T, dPO)\n",
    "            Wj_grad += np.dot(X[t:t+1].T, dPJ)\n",
    "            Wi_grad += np.dot(X[t:t+1].T, dPI)\n",
    "            Wf_grad += np.dot(X[t:t+1].T, dPF)\n",
    "            \n",
    "            # Likewise, with bias gradients\n",
    "            Bo_grad += dPO\n",
    "            Bj_grad += dPJ\n",
    "            Bi_grad += dPI\n",
    "            Bf_grad += dPF\n",
    "            \n",
    "            # Likewise, with hidden state to gate matrices\n",
    "            if t != 0:\n",
    "                Uo_grad += np.dot(H[t - 1].T, dPO)\n",
    "                Uj_grad += np.dot(H[t - 1].T, dPJ)\n",
    "                Ui_grad += np.dot(H[t - 1].T, dPI)\n",
    "                Uf_grad += np.dot(H[t - 1].T, dPF)\n",
    "                \n",
    "            # Compute gradients of final layer\n",
    "            W2_grad += np.dot(H[t].T, dPYh)\n",
    "            B2_grad += dPYh\n",
    "            \n",
    "        gradients = []\n",
    "            \n",
    "        # Normalize gradients by dividing by sequence length and\n",
    "        # store them in list\n",
    "        for theta in (Wf_grad, Wi_grad, Wj_grad, Wo_grad, W2_grad, Bf_grad, Bi_grad, Bj_grad, Bo_grad, B2_grad,\n",
    "                      Uf_grad, Ui_grad, Uj_grad, Uo_grad):\n",
    "            gradients.append(theta / len(X))\n",
    "            \n",
    "        # Return normalized error and gradients\n",
    "        return error / len(X), tuple(gradients), Hp, Cp\n",
    "    \n",
    "    # Applying RMSProp: an optimization strategy to adjust weights and \n",
    "    # biases similar to Adagrad, without decaying the learning rate \n",
    "    # significantly more than necessary\n",
    "    def apply_rmsprop(self, gradients, gamma):\n",
    "        \n",
    "        # Set params to list of all weights and biases\n",
    "        theta = [self.Wf, self.Wi, self.Wj, self.Wo, self.W2, self.Bf, self.Bi,\n",
    "                  self.Bj, self.Bo, self.B2, self.Uf, self.Ui, self.Uj, self.Uo]\n",
    "        \n",
    "        # Iterate through parameter matrices with memory\n",
    "        # and gradients\n",
    "        for i in range(len(theta)):\n",
    "            \n",
    "            self.memory[i] = gamma * self.memory[i] + (1 - gamma) * (gradients[i] * gradients[i])\n",
    "            \n",
    "            theta[i] += -self.learning_rate * gradients[i] / np.sqrt(self.memory[i] + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "text = open(\"eminem.txt\", 'r').read()\n",
    "len_text = len(text)\n",
    "\n",
    "# Creating a list that contains every unique character\n",
    "in_to_char = sorted(list(set(text)))\n",
    "\n",
    "# Total unique characters\n",
    "unique_chars = len(in_to_char)\n",
    "\n",
    "# Dictionary that maps characters to their respective indices\n",
    "char_to_in = {in_to_char[i]:i for i in range(len(in_to_char))}\n",
    "\n",
    "# Function retrieving next sequence of data\n",
    "current_in = 0\n",
    "def next_batch(seq_length):\n",
    "    # Update index to get different text every time\n",
    "    global current_in\n",
    "    if current_in + seq_length + 1 >= len_text:\n",
    "        current_in = 0\n",
    "    \n",
    "    # Get indices, set y to the next x\n",
    "    x_in = [char_to_in[x] for x in text[current_in:current_in+seq_length]]\n",
    "    y_in = [char_to_in[x] for x in text[current_in+1:current_in+seq_length+1]]\n",
    "    \n",
    "    x_1hot = np.zeros([seq_length, unique_chars])\n",
    "    y_1hot = np.zeros([seq_length, unique_chars])\n",
    "    \n",
    "    # Convert indices into 1hot\n",
    "    for i in range(len(x_in)):\n",
    "        x_1hot[i][x_in[i]] = 1\n",
    "        y_1hot[i][y_in[i]] = 1\n",
    "        \n",
    "    current_in += seq_length\n",
    "        \n",
    "    return x_1hot, y_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 200\n",
    "learning_rate = 0.002\n",
    "epochs = 60000\n",
    "hidden_dim = 100\n",
    "display_step = 5\n",
    "gamma = 0.9\n",
    "\n",
    "# Initializing network object\n",
    "lstm = LSTM(learning_rate, unique_chars, hidden_dim, unique_chars)\n",
    "\n",
    "h_prev, c_prev = [np.zeros([1, hidden_dim])] * 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Get next sequence\n",
    "    x_1hot, y_1hot = next_batch(seq_length)\n",
    "    \n",
    "    # Train network and update parameters after each sequence\n",
    "    error, gradients, h_prev, c_prev = lstm.compute_gradients(x_1hot, y_1hot, h_prev, c_prev)\n",
    "    lstm.apply_rmsprop(gradients, gamma)\n",
    "    \n",
    "    if epoch % display_step == 0:\n",
    "        raw = lstm.sample(h_prev, c_prev, char_to_in['T'])\n",
    "        print(\"Epoch\", epoch, \"Error\", error)\n",
    "        print(\"\\n\\n\", \"\".join(in_to_char[i] for i in raw), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2000 character sample after training\n",
    "\n",
    "sample = lstm.sample(np.zeros([1, hidden_dim]), np.zeros([1, hidden_dim]), char_to_in['I'], length=2000)\n",
    "print(\"\\n\\n\", \"\".join(in_to_char[i] for i in sample), \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
